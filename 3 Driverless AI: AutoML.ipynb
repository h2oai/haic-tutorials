{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driverless AI for AutoML\n",
    "\n",
    "This notebook is intended to help you get started with AutoML in the H2O AI Cloud using python.\n",
    "\n",
    "* **Product Documentation:** https://docs.h2o.ai/driverless-ai/1-10-lts/docs/userguide/index.html\n",
    "* **Python Documentation:** https://docs.h2o.ai/driverless-ai/pyclient/docs/html/index.html\n",
    "* **Additional Tutorials:** https://github.com/h2oai/driverlessai-tutorials/tree/master/dai_python_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o_engine_manager\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Securely connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_manager = h2o_engine_manager.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Driverless AI\n",
    "We'll create a connection object called dai that we will use to interact with the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dai_engine = engine_manager.dai_engine_client.create_engine(\n",
    "    display_name=\"My-Tutorial-Engine-03\",\n",
    ")\n",
    "\n",
    "dai_engine.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f287537",
   "metadata": {},
   "outputs": [],
   "source": [
    "dai = dai_engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visit the DAI URL\n",
    "You must click the link below if you would like to use the DAI UI anywhere else in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dai_engine.login_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dai.server.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "You can get links to the home page and search the documentation from the python client any time you want to know more about a specific DAI functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dai.server.docs(\"autoviz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Various methods for loading, interacting with, and modifiying data wihthin Driverless AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Available Connectors\n",
    "View all ways you are allowed to add data to your DAI instace - to enable more connectors reach out to support@h2o.ai <br/><br/>\n",
    "**Note:** Interactions with Data Recipes is not yet available so these connectors are not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dai.connectors.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Existing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a link to view all datasets in the UI\n",
    "dai.datasets.gui()\n",
    "\n",
    "print()\n",
    "\n",
    "# List all datasets\n",
    "print(dai.datasets.list(start_index=0, count=4))\n",
    "\n",
    "print()\n",
    "\n",
    "# pretty print\n",
    "for d in dai.datasets.list(start_index=0, count=4):\n",
    "    print(type(d), d.key, d.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload and Download Data\n",
    "You can upload data using any method that is enabled on your system. Here we will show:\n",
    "* Add data from a public S3 bucket\n",
    "* Upload data from your local machine\n",
    "* Download a dataset\n",
    "* Rename a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_churn = dai.datasets.create(data=\"https://h2o-internal-release.s3-us-west-2.amazonaws.com/data/Splunk/churn.csv\",\n",
    "                                  data_source=\"s3\",\n",
    "                                  name=\"Telco_Churn\",\n",
    "                                  force=True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_path = telco_churn.download(\"./\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "telco_churn2 = dai.datasets.create(local_file_path, name=\"Telco_Churn_Duplicate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old Name:\", telco_churn2.name)\n",
    "\n",
    "telco_churn2.rename(\"Fancy New Name\")\n",
    "\n",
    "print(\"New Name:\", telco_churn2.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Dataset\n",
    "* View the column names\n",
    "* View the data shape\n",
    "* View the first and last rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telco_churn.key, \"|\", telco_churn.name)\n",
    "print(\"\\nColumns:\", telco_churn.columns)\n",
    "print('\\nShape:', telco_churn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_churn.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Columns\n",
    "* View column summaries\n",
    "* Update the datatype to be used in modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all column summaries\n",
    "# print(telco_churn.column_summaries())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telco_churn.column_summaries([\"Area Code\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force a numeric column to only be used as a category\n",
    "telco_churn.set_logical_types({'Area Code': ['categorical']})\n",
    "print(telco_churn.column_summaries()[\"Area Code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split a Dataset\n",
    "The split function returns a dictionary of two datasets so you can easily pass them to the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_churn_split = telco_churn.split_to_train_test(\n",
    "    train_size=0.8,\n",
    "    train_name='telco_churn_train',\n",
    "    test_name='telco_churn_test',\n",
    "    target_column= \"Churn?\", # Beta users with client from before March 15th use target_col\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_churn_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in telco_churn_split.items():\n",
    "    print(k, \"\\t\" ,v.key, v.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipes\n",
    "Recipes are components of the ML pipeline such as algorithms, feature transformers, and scores. You can view all availabe recipes and upload new ones to your instance of DAI.\n",
    "\n",
    "* List all available models\n",
    "* List any custom transformers\n",
    "* List all scorers that can be used for binomial classification\n",
    "* Upload a custom recipe and save it to be tested later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[m.name for m in dai.recipes.models.list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.name for t in dai.recipes.transformers.list() if t.is_custom ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s.name for s in dai.recipes.scorers.list() if s.for_binomial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dai.recipes.create(\"https://github.com/h2oai/driverlessai-recipes/blob/rel-1.8.6/transformers/numeric/sum.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.name for t in dai.recipes.transformers.list() if t.is_custom ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_to_use = [t for t in dai.recipes.transformers.list() if not t.is_custom or 'Sum' in t.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "**Notes:** Dictionaries allow you to easily use common settings in your experiments <br/>\n",
    "**Notes:** Experiments will be `sync` by default meaning they will lock the notebook until they are complete. You can also use `async` versions of the fucntions. With the `async` functions you can use included code below to monthior and experiment as it runs, see logs in real time, and stop it when it is \"good enough\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Existing Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e.name for e in dai.experiments.list()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary for a Use Case\n",
    "We might want to run several experiments with different dial and expert settings. All of these will likely have some things in common, namely details about this specific dataset. We will create a dictionary to use in many experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_settings = {\n",
    "    **telco_churn_split,\n",
    "    'task': 'classification',\n",
    "    'target_column': \"Churn?\", # Beta users with client from before March 15th use target_col\n",
    "    'scorer': 'F1'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary for Fast Experiments\n",
    "\n",
    "There may be several common types of experiments you want to run, and H2O.ai will be creating common experiment settings in dictionaries for easy use. The one below turns off all extra settings such as building pipelines or checking for leakage. It also uses the fastest experiment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_settings = {\n",
    "    'accuracy': 1,\n",
    "    'time': 1,\n",
    "    'interpretability': 6,\n",
    "    'make_python_scoring_pipeline': 'off',\n",
    "    'make_mojo_scoring_pipeline': 'off',\n",
    "    'benchmark_mojo_latency': 'off',\n",
    "    'make_autoreport': False,\n",
    "    'check_leakage': 'off',\n",
    "    'check_distribution_shift': 'off'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for Settings\n",
    "There are many expert settings available, you can use the serach functionality to look for names or keywords for settings you may want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dai.experiments.search_expert_settings('imbalanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dai.experiments.search_expert_settings('imbalanced', show_description=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Recommended Dial Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment preview with our settings\n",
    "dai.experiments.preview(\n",
    "    **telco_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Using Our Custom Recipe\n",
    "Notice that `Sum` has been added to the `Feature engineering search space`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get experiment preview with our settings\n",
    "dai.experiments.preview(\n",
    "    **telco_settings\n",
    "    ,transformers=transformers_to_use\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch an Experiment\n",
    "We will start by running an async experiment which will immeadiatly free our notebook to run additional commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_baseline = dai.experiments.create_async(\n",
    "    **telco_settings,\n",
    "    name='Fastest Settings', **fast_settings,\n",
    "    force=True\n",
    "    # name='Default Baseline', accuracy=7, time=2, interpretability=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information on an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Name:\", default_baseline.name)\n",
    "print(\"Datasets:\", default_baseline.datasets)\n",
    "print(\"Target:\", default_baseline.settings['target_column']) # beta users from before March 15th use target_col\n",
    "print(\"Scorer:\", default_baseline.metrics()['scorer'])\n",
    "print(\"Task:\", default_baseline.settings['task'])\n",
    "print(\"Status:\", default_baseline.status(verbose=2))\n",
    "print(\"Web Page: \", end='')\n",
    "default_baseline.gui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor and Finish the Model Early\n",
    "Example of how you may want to monitor a running experiment, this will print the currently logs and accuracy metrics. You can also finish a model early if it reaches a certain accuracy metric or run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the experiment and stop at a nice model\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "while default_baseline.is_running():\n",
    "    time.sleep(1)\n",
    "\n",
    "    # grab experiment status\n",
    "    status = default_baseline.status(verbose=2)\n",
    "\n",
    "    # grab current metrics\n",
    "    metrics = default_baseline.metrics()\n",
    "\n",
    "    # pretty print info\n",
    "    clear_output(wait=True)\n",
    "    print(status, \" - Validation \", metrics['scorer'], \": \", sep='', end='')\n",
    "\n",
    "    if metrics['val_score'] is not None:\n",
    "        print(round(metrics['val_score'], 4), '+/-', round(metrics['val_score_sd'], 4))\n",
    "        if metrics['val_score'] > 0.9:\n",
    "            default_baseline.finish()\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    default_baseline.log.tail(3)\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\nTest \", default_baseline.metrics()['scorer'], \": \",\n",
    "      round(default_baseline.metrics()['test_score'], 4), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_baseline.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with Model Artifacts\n",
    "* See which are available\n",
    "* Create the AutoReport\n",
    "* Download the AutoReport\n",
    "* Open the AutoReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available artifacts:\", default_baseline.artifacts.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_baseline.artifacts.create('autoreport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = default_baseline.artifacts.download(['autoreport'], \"./\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Final Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_baseline.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation\", default_baseline.metrics()[\"scorer\"], \":\\t\",round(default_baseline.metrics()['val_score'], 3))\n",
    "print(\"Test\", default_baseline.metrics()[\"scorer\"], \":\\t\",round(default_baseline.metrics()['test_score'], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download predictions from test dataset\n",
    "artifacts = default_baseline.artifacts.download(['test_predictions'], \"./\", overwrite=True)\n",
    "local_predictions = pd.read_csv(artifacts['test_predictions'])\n",
    "\n",
    "local_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC curve\n",
    "* Download the predictions with the Actual column\n",
    "* Use sklearn to calculate ROC curve\n",
    "* Plot AUC & ROC Curve by a categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = default_baseline.predict(telco_churn_split['test_dataset'],\n",
    "                         include_columns=[\"Churn?\", \"Area Code\"])\n",
    "\n",
    "test_predictions = pd.read_csv(preds.download(\"./\", dst_file=\"test_predictions.csv\"))\n",
    "\n",
    "test_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "test_predictions[\"Actual\"] = np.where(test_predictions[\"Churn?\"] == \"True.\", 1, 0)\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(test_predictions[\"Actual\"], [0 for _ in range(len(test_predictions[\"Churn?\"]))])\n",
    "lr_fpr, lr_tpr, _ = roc_curve(test_predictions[\"Actual\"], test_predictions[\"Churn?.True.\"])\n",
    "\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ac in test_predictions[\"Area Code\"].unique():\n",
    "    grp = test_predictions[test_predictions[\"Area Code\"] == ac]\n",
    "\n",
    "    # calculate roc curves\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(grp[\"Actual\"], [0 for _ in range(len(grp[\"Churn?\"]))])\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(grp[\"Actual\"], grp[\"Churn?.True.\"])\n",
    "\n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "\n",
    "    pyplot.title('AUC of Area Code ' + str(ac) + ': ' + str(round(auc(lr_fpr, lr_tpr), 3)))\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_baseline.variable_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain a Model for Production\n",
    "* Retrain the final model with the full dataset\n",
    "* Print the model metrics\n",
    "* Create and Download the MOJO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dai.experiments.search_expert_settings(\"mojo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain on all data for productionalizing\n",
    "full_model = default_baseline.retrain(final_pipeline_only=True,\n",
    "                                      train_dataset=telco_churn,\n",
    "                                      test_dataset=\"\",\n",
    "                                      make_mojo_scoring_pipeline=\"on\")\n",
    "\n",
    "full_model = dai.experiments.get(full_model.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = full_model.artifacts.download(\"mojo_pipeline\", './', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dai_engine.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
